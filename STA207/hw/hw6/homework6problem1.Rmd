---
title: "STA207 homework6"
author: "Juanjuan Hu, Zhen Zhang"
date: "March 5, 2016"
output: pdf_document
---
# 1
```{r,echo=FALSE,warning=FALSE,cache=FALSE,cache.lazy=TRUE}
library(xlsx)
library(car)
library(MASS)
library(ridge)
setwd("/Users/PullingCarrot/Desktop/201601-03/sta207/homework/hw6")
apartment = read.xlsx2("apartment.xlsx",1)
```

## (a)
```{r}
apartments=data.frame(sapply(apartment, function(x) scale(as.numeric(as.character(x)))))
pairs(apartments)
cor(apartments) # x2 and x5 shows linear pattern
```
From the matrix plot of data and the correlation matrix, we can find that some independent variables seem to be correlated, like X2 and X5. It seems that multicollinearity is present. 

## (b)
```{r}
X = as.matrix(apartments[,2:6])
eigen(t(X)%*%X)$values
```
We can see that the maximum eigenvalue is 63.22 and the smallest eigenvalue is 1.99. The condition index (maximum divided by the minumum) is 31.78, which is relatively large. It indicates the existance of multicollinearity.

## (c)
```{r}
fit1=lm(apartments$Y~0+X)
summ.fit1=summary(fit1)
summ.fit1
anova(fit1)
```
From the regression result, we can see that the the R-square is 0.98, meaning that the five variables could explain the response variable well. The p-value of the F test is 2.2e-16, indicating that the not all the coefficients are equal to zero.

## (d)
```{r}
sapply(1:5, function(i) 1/(1-summary(lm(X[,i]~0+X[,-i]))$r.squared) )
```
The largest VIF is the VIF for variable X2, which is 7.66. It means that X2 can be partially linear represented by other variables, indicating presence of multicollinearity. 

## (e)
```{r}
fit.ridge = lm.ridge(apartments$Y~0+X, lambda=seq(0,1,by=0.001))
lambda=lambda=seq(0,1,by=0.001)
GCV = fit.ridge$GCV
lambda.opt=lambda[which.min(GCV)] # lambda=0.321
lambda.opt
plot(x=lambda, y=GCV)
```
The optimal penalty parameter is 0.321.

```{r}
fit2 = lm.ridge(apartments$Y~0+X,lambda=lambda.opt)
beta.hat=fit2$coef
y.hat=X%*%beta.hat
summ.fit2=summary(linearRidge(apartments$Y~0+X,lambda=lambda.opt,scaling="scale"))
summ.fit2
```

## (f)
```{r}
par(mfrow=c(1,3),mar=c(4,4,2,2))
plot(x=apartments$Y, y=y.hat, main = "Observed response \nagainst the fitted values", cex.main=1, xlab="observed", ylab="fitted")
res.fit2 = apartments$Y-y.hat
plot(x=y.hat,y = res.fit2, main = "Residuals against the\n fitted values", xlab="fitted", ylab="residuals",cex.main=1)
qqnorm(res.fit2)
qqline(res.fit2)
```
From the residuals plot, we can see that there is no obvious pattern within the residuals. And the residuals do not show serious departure from normality.

## (g)
```{r}
R=t(X)%*%X
I=diag(nrow=5)
M=solve(R+lambda.opt*I)
A=M%*%R%*%M
diag(solve(A))*diag(A)
```
Compared to the VIF obtained in part(d), the VIF for the estimated ridge regression is smaller. It indicates that the problem of multicollinearity is remedied a bit.